{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import heapq\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "MAX_LENGTH = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.char2index = {}\n",
    "        self.char2count = {}\n",
    "        self.index2char = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_chars = 2  # Count SOS and EOS\n",
    "\n",
    "    def addWord(self, word):\n",
    "        for char in word:\n",
    "            self.addChar(char)\n",
    "\n",
    "    def addChar(self, char):\n",
    "        if char not in self.char2index:\n",
    "            self.char2index[char] = self.n_chars\n",
    "            self.char2count[char] = 1\n",
    "            self.index2char[self.n_chars] = char\n",
    "            self.n_chars += 1\n",
    "        else:\n",
    "            self.char2count[char] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLangs(lang, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    lines = open('aksharantar_sampled/%s/%s_train.csv' % (lang, lang), encoding='utf-8').read().strip().split('\\n')\n",
    "    \n",
    "    # Split every line into pairs and normalize\n",
    "    # pairs = [[normalizeString(s) for s in l.split(',')] for l in lines]\n",
    "    pairs = [[s for s in l.split(',')] for l in lines]\n",
    "\n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang)\n",
    "        output_lang = Lang('eng')\n",
    "    else:\n",
    "        input_lang = Lang('eng')\n",
    "        output_lang = Lang(lang)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 51200 word pairs\n",
      "Trimmed to 51200 word pairs\n",
      "Counting chars...\n",
      "Counted chars:\n",
      "eng 28\n",
      "hin 66\n",
      "['chaheiti', 'चहेती']\n"
     ]
    }
   ],
   "source": [
    "def prepareData(lang, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLangs(lang, reverse)\n",
    "    print(\"Read %s word pairs\" % len(pairs))\n",
    "    print(\"Trimmed to %s word pairs\" % len(pairs))\n",
    "    print(\"Counting chars...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addWord(pair[0])\n",
    "        output_lang.addWord(pair[1])\n",
    "    print(\"Counted chars:\")\n",
    "    print(input_lang.name, input_lang.n_chars)\n",
    "    print(output_lang.name, output_lang.n_chars)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData('hin', False)\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "cells = {'rnn': nn.RNN, 'gru': nn.GRU, 'lstm': nn.LSTM}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Class for the encoder RNN.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, num_hidden_layers, dropout=0, cell_type='gru'):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.cell_type = cell_type\n",
    "        self.cell = cells[cell_type](hidden_size, hidden_size, num_hidden_layers, dropout=dropout)\n",
    "\n",
    "    def forward(self, input, hidden, cell=None):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        if self.cell_type == 'rnn' or self.cell_type == 'gru':\n",
    "            output, hidden = self.cell(embedded, hidden)\n",
    "            return output, hidden\n",
    "        else:\n",
    "            output, (hidden, cell) = self.cell(embedded, (hidden, cell))    \n",
    "            return output, hidden, cell\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BeamSearchNode:\n",
    "    def __init__(self, decoder_output, hidden, prev_node, char_idx, log_prob, length):\n",
    "        self.decoder_output = decoder_output\n",
    "        self.hidden = hidden\n",
    "        self.prev_node = prev_node\n",
    "        self.char_idx = char_idx\n",
    "        self.log_prob = log_prob\n",
    "        self.length = length\n",
    "\n",
    "    def __lt__(self, other):\n",
    "        return self.log_prob > other.log_prob\n",
    "\n",
    "    def eval(self):\n",
    "        return self.log_prob / self.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Class for the vanilla decoder RNN.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size, output_size, num_hidden_layers, dropout=0, cell_type='gru'):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.cell_type = cell_type\n",
    "        self.cell = cells[cell_type](hidden_size, hidden_size, num_hidden_layers, dropout=dropout)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.output_size = output_size\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden, cell=None):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        if self.cell_type == 'rnn' or self.cell_type == 'gru':\n",
    "            output, hidden = self.cell(output, hidden)\n",
    "        else:\n",
    "            output, (hidden, cell) = self.cell(output, (hidden, cell))\n",
    "            \n",
    "        output = self.softmax(self.out(output[0]))\n",
    "\n",
    "        if self.cell_type == 'rnn' or self.cell_type == 'gru':\n",
    "            return output, hidden\n",
    "        else:\n",
    "            return output, hidden, cell\n",
    "    \n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "\n",
    "    def beam_search(self, encoder_outputs, decoder_hidden, beam_width, start_token, end_token):\n",
    "        # Initialize the beam search\n",
    "        beam_nodes = [BeamSearchNode(None, decoder_hidden, None, start_token, 0, 1)]\n",
    "        done_nodes = []\n",
    "\n",
    "        # Keep expanding the beam until we reach the maximum length or all candidates are done\n",
    "        for _ in range(MAX_LENGTH):\n",
    "            candidates = []\n",
    "            for node in beam_nodes:\n",
    "                if node.char_idx == end_token:\n",
    "                    done_nodes.append(node)\n",
    "                    continue\n",
    "\n",
    "                # Feed the previous char and hidden state into the decoder\n",
    "                output, hidden = self.forward(torch.tensor([[node.char_idx]]), node.hidden, encoder_outputs)\n",
    "                # Single depth or double depth for tensor?\n",
    "\n",
    "                # Generate new candidate nodes and add them to the heap\n",
    "                for char_idx in range(self.output_size):\n",
    "                    length = node.length + 1\n",
    "                    # print(output, node.log_prob)\n",
    "                    log_prob = node.log_prob + output[0][char_idx].item()\n",
    "                    # print(log_prob)\n",
    "                    new_node = BeamSearchNode(output, hidden, node, char_idx, log_prob, length)\n",
    "                    heapq.heappush(candidates, new_node)\n",
    "\n",
    "            # Select the top k candidates to continue expanding the beam\n",
    "            beam_nodes = []\n",
    "            for _ in range(beam_width):\n",
    "                if not candidates:\n",
    "                    break\n",
    "                beam_nodes.append(heapq.heappop(candidates))\n",
    "\n",
    "            if not beam_nodes:\n",
    "                break\n",
    "\n",
    "        # Return the best candidate as the predicted sequence\n",
    "        done_nodes.extend(beam_nodes)\n",
    "        best_node = max(done_nodes, key=lambda node: node.eval())\n",
    "        # predicted_seq = []\n",
    "        # while best_node.prev_node is not None:\n",
    "        #     predicted_seq.append(best_node.char_idx)\n",
    "        #     best_node = best_node.prev_node\n",
    "        # return predicted_seq[::-1]\n",
    "        return best_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromWord(lang, word):\n",
    "    return [lang.char2index[char] for char in word]\n",
    "\n",
    "\n",
    "def tensorFromWord(lang, word):\n",
    "    indexes = indexesFromWord(lang, word)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromWord(input_lang, pair[0])\n",
    "    target_tensor = tensorFromWord(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, cell_type, encoder_optimizer, decoder_optimizer, criterion, beam_width, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    \n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    if cell_type == 'lstm':\n",
    "        encoder_cell = encoder.initHidden()\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        if cell_type == 'lstm':\n",
    "            encoder_output, encoder_hidden, encoder_cell = encoder(input_tensor[ei], encoder_hidden, encoder_cell)\n",
    "        else:\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    if beam_width == 1:\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "        if cell_type == 'lstm':\n",
    "            decoder_cell = encoder_cell\n",
    "\n",
    "        use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "        if use_teacher_forcing:\n",
    "            # Teacher forcing: Feed the target as the next input\n",
    "            for di in range(target_length):\n",
    "                # decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "                if cell_type == 'lstm':\n",
    "                    decoder_output, decoder_hidden, decoder_cell = decoder(decoder_input, decoder_hidden, decoder_cell)\n",
    "                else:\n",
    "                    decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "                print(\"Criterion:\", criterion(decoder_output, target_tensor[di])) \n",
    "                loss += criterion(decoder_output, target_tensor[di])\n",
    "                print(loss)\n",
    "                decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "        else:\n",
    "            # Without teacher forcing: use its own predictions as the next input\n",
    "            for di in range(target_length):\n",
    "                # decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "                if cell_type == 'lstm':\n",
    "                    decoder_output, decoder_hidden, decoder_cell = decoder(decoder_input, decoder_hidden, decoder_cell)\n",
    "                else:\n",
    "                    decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "                topv, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "                print(\"Criterion:\", criterion(decoder_output, target_tensor[di])) \n",
    "                loss += criterion(decoder_output, target_tensor[di])\n",
    "                print(loss)\n",
    "                if decoder_input.item() == EOS_token:\n",
    "                    break\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "\n",
    "        return loss.item() / target_length\n",
    "    \n",
    "    else:\n",
    "        predicted_seq = decoder.beam_search(encoder_outputs, decoder_hidden, beam_width, SOS_token, EOS_token)\n",
    "        # loss = sum([criterion(decoder_output, target_tensor[di]) for di, decoder_output in enumerate(predicted_seq)])\n",
    "        loss = 0\n",
    "        # assert len(predicted_seq) == target_length, \"Predicted sequence length {} does not match target sequence length {}\".format(len(predicted_seq), target_length)\n",
    "        for di, char_idx in enumerate(predicted_seq):\n",
    "            if di == target_length:\n",
    "                break\n",
    "            decoder_input = torch.tensor([[char_idx]], device=device)\n",
    "            if cell_type == 'lstm':\n",
    "                decoder_output, decoder_hidden, decoder_cell = decoder(decoder_input, decoder_hidden, decoder_cell)\n",
    "            else:\n",
    "                decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "\n",
    "        # print(loss)\n",
    "        loss.backward()\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "        return loss.item() / len(predicted_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, optimizer=optim.SGD, cell_type='gru', beam_width=1, print_every=1000, learning_rate=5e-3):\n",
    "    start = time.time()\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "\n",
    "    encoder_optimizer = optimizer(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optimizer(decoder.parameters(), lr=learning_rate)\n",
    "    training_pairs = [tensorsFromPair(random.choice(pairs)) for _ in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "\n",
    "        loss = train(input_tensor, target_tensor, encoder, decoder, cell_type, encoder_optimizer, decoder_optimizer, criterion, beam_width)\n",
    "        print_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters), iter, iter / n_iters * 100, print_loss_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, word, beam_width=1, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromWord(input_lang, word)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        if encoder.cell_type == 'lstm':\n",
    "            encoder_cell = encoder.initHidden()\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            if encoder.cell_type == 'lstm':\n",
    "                encoder_output, encoder_hidden, encoder_cell = encoder(input_tensor[ei], encoder_hidden, encoder_cell)\n",
    "            else:\n",
    "                encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "                \n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        if beam_width == 1:\n",
    "            decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "\n",
    "            if encoder.cell_type == 'lstm':\n",
    "                decoder_cell = encoder_cell\n",
    "\n",
    "            decoded_chars = \"\"\n",
    "            # decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "            for di in range(max_length):\n",
    "                # decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "                if encoder.cell_type == 'lstm':\n",
    "                    decoder_output, decoder_hidden, decoder_cell = decoder(decoder_input, decoder_hidden, decoder_cell)\n",
    "                else:\n",
    "                    decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "                    \n",
    "                # decoder_attentions[di] = decoder_attention.data\n",
    "                topv, topi = decoder_output.data.topk(1)\n",
    "                if topi.item() == EOS_token:\n",
    "                    decoded_chars += '<EOS>'\n",
    "                    break\n",
    "                else:\n",
    "                    decoded_chars += output_lang.index2char[topi.item()]\n",
    "\n",
    "                decoder_input = topi.squeeze().detach()\n",
    "\n",
    "            # return decoded_chars, decoder_attentions[:di + 1]\n",
    "            return decoded_chars\n",
    "        \n",
    "        else:\n",
    "            predicted_seq = decoder.beam_search(encoder_outputs, decoder_hidden, beam_width, SOS_token, EOS_token)\n",
    "            return ''.join([output_lang.index2char[char_idx] for char_idx in predicted_seq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        # output_chars, attentions = evaluate(encoder, decoder, pair[0])\n",
    "        output_word = evaluate(encoder, decoder, pair[0])\n",
    "        # output_word = ''.join(output_chars)\n",
    "        print('<', output_word)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 256\n",
    "encoder1 = EncoderRNN(input_lang.n_chars, hidden_size, num_hidden_layers=1).to(device)\n",
    "decoder1 = DecoderRNN(hidden_size, output_lang.n_chars, num_hidden_layers=1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 27s (- 4m 11s) (100 10%) 3.0889\n",
      "0m 50s (- 3m 20s) (200 20%) 2.8824\n",
      "1m 10s (- 2m 44s) (300 30%) 3.2302\n",
      "1m 31s (- 2m 17s) (400 40%) 3.0131\n",
      "1m 50s (- 1m 50s) (500 50%) 3.3526\n",
      "2m 10s (- 1m 26s) (600 60%) 3.3453\n",
      "2m 28s (- 1m 3s) (700 70%) 3.3008\n",
      "2m 45s (- 0m 41s) (800 80%) 3.3673\n",
      "3m 3s (- 0m 20s) (900 90%) 3.2582\n",
      "3m 21s (- 0m 0s) (1000 100%) 3.2475\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, decoder1, n_iters=1000, beam_width=5, print_every=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> nirgunmargi\n",
      "= निर्गुणमार्गी\n",
      "< पवररिरा<EOS>\n",
      "\n",
      "> sairaaz\n",
      "= सैराज़\n",
      "< पवररिर<EOS>\n",
      "\n",
      "> kalhaili\n",
      "= कलहैली\n",
      "< पवररिर<EOS>\n",
      "\n",
      "> sambhave\n",
      "= सम्भवे\n",
      "< पवरर्<EOS>\n",
      "\n",
      "> sattachyut\n",
      "= सत्ताच्यूत\n",
      "< सवरर्<EOS>\n",
      "\n",
      "> vilima\n",
      "= विलिमा\n",
      "< सवरर्<EOS>\n",
      "\n",
      "> nilakantan\n",
      "= नीलाकांतन\n",
      "< ववरिररा<EOS>\n",
      "\n",
      "> baalo\n",
      "= बालों\n",
      "< सवरर्<EOS>\n",
      "\n",
      "> trogir\n",
      "= ट्रोगिर\n",
      "< पवररिर<EOS>\n",
      "\n",
      "> coric\n",
      "= कोरिच\n",
      "< पवररिर<EOS>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluateRandomly(encoder1, decoder1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deligation\n",
      "डेलिगेशन\n",
      "सरिरर्ाEOS\n"
     ]
    }
   ],
   "source": [
    "pair = random.choice(pairs)\n",
    "\n",
    "print(pair[0])\n",
    "print(pair[1])\n",
    "\n",
    "with torch.no_grad():\n",
    "    input_tensor = tensorFromWord(input_lang, pair[0])\n",
    "    input_length = input_tensor.size()[0]\n",
    "    encoder1_hidden = encoder1.initHidden()\n",
    "\n",
    "    encoder1_outputs = torch.zeros(MAX_LENGTH, encoder1.hidden_size, device=device)\n",
    "\n",
    "    if encoder1.cell_type == 'lstm':\n",
    "        encoder1_cell = encoder1.initHidden()\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        if encoder1.cell_type == 'lstm':\n",
    "            encoder1_output, encoder1_hidden, encoder1_cell = encoder1(input_tensor[ei], encoder1_hidden, encoder1_cell)\n",
    "        else:\n",
    "            encoder1_output, encoder1_hidden = encoder1(input_tensor[ei], encoder1_hidden)\n",
    "            \n",
    "        encoder1_outputs[ei] += encoder1_output[0, 0]\n",
    "\n",
    "    decoder1_hidden = encoder1_hidden\n",
    "\n",
    "    if encoder1.cell_type == 'lstm':\n",
    "        decoder1_cell = encoder1_cell\n",
    "\n",
    "    decoded_chars = \"\"\n",
    "\n",
    "    # Predict the output using beam search\n",
    "    predicted_seq = decoder1.beam_search(encoder1_outputs, decoder1_hidden, 5, SOS_token, EOS_token)\n",
    "\n",
    "    # Convert the predicted sequence to a word\n",
    "    for char_idx in predicted_seq:\n",
    "        decoded_chars += output_lang.index2char[char_idx]\n",
    "\n",
    "    print(decoded_chars)\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
